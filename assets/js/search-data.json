{
  
    
        "post0": {
            "title": "LIBRARY INSTALLATION",
            "content": "%%capture !pip install musdb !pip install asteroid !git clone https://github.com/speechbrain/speechbrain.git %cd speechbrain !pip install -r requirements.txt !pip install --editable . import speechbrain as sb from speechbrain.lobes.models.dual_path import SepformerWrapper import torch import torch.nn as nn import torch.utils.data as data_utils from torch.utils.data import Dataset, DataLoader from torch.utils.data.dataloader import default_collate import torch.nn.functional as F import torchaudio import musdb from mir_eval.separation import bss_eval_sources import librosa from asteroid.data import MUSDB18Dataset import os import numpy as np import matplotlib.pyplot as plt from functools import partial from random import shuffle from tqdm import tqdm import csv import logging from pathlib import Path os.environ[&#39;MUSDB_PATH&#39;] = &#39;/content&#39; . TRAINING . Load Dataset . Augmentation . class Compose(object): &quot;&quot;&quot;Composes several augmentation transforms. Args: augmentations: list of augmentations to compose. &quot;&quot;&quot; def __init__(self, transforms): self.transforms = transforms def __call__(self, audio): for transform in self.transforms: audio = transform(audio) return audio def _augment_gain(audio, low=0.25, high=1.25): &quot;&quot;&quot;Applies a random gain to each source between `low` and `high`&quot;&quot;&quot; gain = low + torch.rand(1) * (high - low) return audio * gain def _augment_channelswap(audio): &quot;&quot;&quot;Randomly swap channels of stereo sources&quot;&quot;&quot; if audio.shape[0] == 2 and torch.FloatTensor(1).uniform_() &lt; 0.5: return torch.flip(audio, [0]) return audio . Training and Validation Dataset object . root_path = Path(&#39;/content/drive/MyDrive/MusDB/wav_8k&#39;) validation_tracks = [ &quot;Actions - One Minute Smile&quot;, &quot;Clara Berry And Wooldog - Waltz For My Victims&quot;, &quot;Johnny Lokke - Promises &amp; Lies&quot;, &quot;Patrick Talbot - A Reason To Leave&quot;, &quot;Triviul - Angelsaint&quot;, &quot;Alexander Ross - Goodbye Bolero&quot;, &quot;Fergessen - Nos Palpitants&quot;, &quot;Leaf - Summerghost&quot;, &quot;Skelpolu - Human Mistakes&quot;, &quot;Young Griffo - Pennies&quot;, &quot;ANiMAL - Rockshow&quot;, &quot;James May - On The Line&quot;, &quot;Meaxic - Take A Step&quot;, &quot;Traffic Experiment - Sirens&quot;, ] train_list = os.listdir(&#39;/content/drive/MyDrive/MusDB/wav_8k/train&#39;) train_subset = [song for song in train_list if song not in validation_tracks] # Augmentation function source_augmentations = Compose([globals()[&quot;_augment_&quot; + aug] for aug in [&#39;gain&#39;, &#39;channelswap&#39;]]) # Load training dataset train_dataset = MUSDB18Dataset(root_path, split=&quot;train&quot;, subset=train_subset, sample_rate=8000, source_augmentations=source_augmentations, random_track_mix=True, segment=6, random_segments=True, samples_per_track=64) # Load validation dataset valid_dataset = MUSDB18Dataset(root_path, split=&quot;train&quot;, subset=validation_tracks, sample_rate=8000, segment=80, random_segments=True) . 100it [02:46, 1.67s/it] 100it [00:29, 3.36it/s] . train_dataset[0] . (tensor([[ 0.0562, 0.0386, 0.1169, ..., -0.0298, -0.0543, -0.0669]]), {&#39;bass&#39;: tensor([[-0.0257, -0.0099, -0.0275, ..., 0.0415, 0.0244, -0.0362]]), &#39;drums&#39;: tensor([[ 0.0807, 0.0624, 0.0333, ..., -0.0144, -0.0070, -0.0173]]), &#39;other&#39;: tensor([[ 0.0206, 0.0039, 0.1332, ..., -0.0688, -0.0831, -0.0255]]), &#39;vocals&#39;: tensor([[-0.0194, -0.0178, -0.0221, ..., 0.0120, 0.0113, 0.0122]])}) . train_dataset[0][0].shape . torch.Size([1, 48000]) . train_dataset[0][1][&#39;vocals&#39;].shape . torch.Size([1, 48000]) . Training and validation DataLoader object . dataloader_kwargs = ( {&quot;num_workers&quot;: 4, &quot;pin_memory&quot;: True} if torch.cuda.is_available() else {} ) train_sampler = torch.utils.data.DataLoader( train_dataset, batch_size=1, shuffle=True, **dataloader_kwargs ) valid_sampler = torch.utils.data.DataLoader(valid_dataset, batch_size=1, **dataloader_kwargs) . Training . MSS framework class . class Separation(sb.Brain): def compute_forward(self, mix, targets, stage, noise=None): &quot;&quot;&quot;Forward computations from the mixture to the separated signals.&quot;&quot;&quot; # Unpack lists and put tensors in the right device mix = mix.to(self.device) # Convert targets to tensor targets = torch.cat( [targets[i].unsqueeze(-1) for i in range(self.hparams.num_spks)], dim=-1, ).to(self.device) print(mix.shape) # Separation mix_w_1 = self.modules.enc(mix) est_mask = self.modules.masker(mix_w_1) mix_w = torch.stack([mix_w_1] * self.hparams.num_spks) sep_h = mix_w * est_mask # Decoding est_source = torch.cat( [ self.modules.dec(sep_h[i]).unsqueeze(-1) for i in range(self.hparams.num_spks) ], dim=-1, ) # T changed after conv1d in encoder, fix it here T_origin = mix.size(1) T_est = est_source.size(1) if T_origin &gt; T_est: est_source = torch.nn.functional.pad(est_source, (0, 0, 0, T_origin - T_est)) else: est_source = est_source[:, :T_origin, :] true_latents = self.modules.enc(targets.view(self.hparams.num_spks, -1)) true_latents = true_latents.view(1, 256, -1, self.hparams.num_spks) est_mix = self.modules.dec(mix_w_1) T_decmix = est_mix.size(1) if T_origin &gt; T_decmix: est_mix = F.pad(est_mix, (0, T_origin - T_est, 0, 0)) else: est_mix = est_mix[:, :T_origin] return est_source, targets, true_latents, est_mix def compute_objectives(self, predictions, targets): &quot;&quot;&quot;Compute SI-SNR loss&quot;&quot;&quot; return sb.nnet.losses.get_si_snr_with_pitwrapper(targets, predictions) def fit_batch(self, batch): &quot;&quot;&quot;Trains one batch&quot;&quot;&quot; # Unpacking batch list mix = batch[0][0] bass = batch[1][&#39;bass&#39;][0] drums = batch[1][&#39;drums&#39;][0] other = batch[1][&#39;other&#39;][0] vocals = batch[1][&#39;vocals&#39;][0] mix, vocals, bass, drums, other = mix.to(self.device), vocals.to(self.device), bass.to(self.device), drums.to(self.device), other.to(self.device) targets = [vocals, bass, drums, other] # Perform source separation predictions, targets, _, _ = self.compute_forward(mix, targets, sb.Stage.TRAIN) # Compute loss loss = self.compute_objectives(predictions, targets) if self.hparams.threshold_byloss: # Loss value clipping th = -30 loss_to_keep = loss[loss &gt; th] if loss_to_keep.nelement() &gt; 0: loss = loss_to_keep.mean() else: loss = loss.mean() # Compute and optimize gradient if ( loss &lt; 999999 and loss.nelement() &gt; 0 ): # the fix for computational problems loss.backward() if 5 &gt;= 0: torch.nn.utils.clip_grad_norm_( self.modules.parameters(), 5 ) self.optimizer.step() else: nonfinite_count += 1 print( &quot;infinite loss or empty loss! it happened {} times so far - skipping this batch&quot;.format( nonfinite_count ) ) loss.data = torch.tensor(0).to(self.device) self.optimizer.zero_grad() return loss.detach().cpu() def evaluate_batch(self, batch, stage): &quot;&quot;&quot;Computations needed for validation/test batches&quot;&quot;&quot; # Unpacking batch list mix = batch[0][0] bass = batch[1][&#39;bass&#39;][0] drums = batch[1][&#39;drums&#39;][0] other = batch[1][&#39;other&#39;][0] vocals = batch[1][&#39;vocals&#39;][0] mix, vocals, bass, drums, other= mix.to(self.device), vocals.to(self.device), bass.to(self.device), drums.to(self.device), other.to(self.device) targets = [vocals, bass, drums, other] with torch.no_grad(): # Perform source separation predictions, targets, _, _ = self.compute_forward( mix, targets, stage) # Compute loss loss = self.compute_objectives(predictions, targets) return loss.detach() def on_stage_end(self, stage, stage_loss, epoch): &quot;&quot;&quot;Gets called at the end of a epoch.&quot;&quot;&quot; # Compute/store important stats stage_stats = {&quot;si-snr&quot;: stage_loss} if stage == sb.Stage.TRAIN: self.train_stats = stage_stats # Perform end-of-iteration things, like annealing, logging, etc. if stage == sb.Stage.VALID: # Learning rate annealing if isinstance( self.hparams.lr_scheduler, sb.nnet.schedulers.ReduceLROnPlateau ): current_lr, next_lr = self.hparams.lr_scheduler( [self.optimizer], epoch, stage_loss ) sb.nnet.schedulers.update_learning_rate(self.optimizer, next_lr) else: # if we do not use the reducelronplateau, we do not change the lr current_lr = self.hparams.optimizer.optim.param_groups[0][&quot;lr&quot;] # Train and validation loss logging self.hparams.train_logger.log_stats( stats_meta={&quot;epoch&quot;: epoch, &quot;lr&quot;: current_lr}, train_stats=self.train_stats, valid_stats=stage_stats) self.checkpointer.save_and_keep_only( meta={&quot;si-snr&quot;: stage_stats[&quot;si-snr&quot;]}, min_keys=[&quot;si-snr&quot;], ) print(&#39;VALID SI-SNR = {}&#39;.format(-stage_loss)) elif stage == sb.Stage.TEST: self.hparams.train_logger.log_stats( stats_meta={&quot;Epoch loaded&quot;: 1}, test_stats=stage_stats, ) def reset_layer_recursively(self, layer): &quot;&quot;&quot;Reinitializes the parameters of the neural networks&quot;&quot;&quot; if hasattr(layer, &quot;reset_parameters&quot;): layer.reset_parameters() for child_layer in layer.modules(): if layer != child_layer: self.reset_layer_recursively(child_layer) . Model and training hyperparameters . tf_blocks = {&#39;SBtfintra&#39;: sb.lobes.models.dual_path.SBTransformerBlock(num_layers=8, d_model=256, nhead=8, d_ffn=1024, dropout=0, use_positional_encoding=True, norm_before=False), &#39;SBtfinter&#39;: sb.lobes.models.dual_path.SBTransformerBlock(num_layers=8, d_model=256, nhead=8, d_ffn=1024, dropout=0, use_positional_encoding=True, norm_before=False),} # Construct the whole MSS model modules = {&quot;enc&quot;: sb.lobes.models.dual_path.Encoder(kernel_size=16, out_channels=256), &quot;masker&quot;: sb.lobes.models.dual_path.Dual_Path_Model(num_spks=4, in_channels=256, out_channels=256, num_layers=2, K=400, intra_model=tf_blocks[&#39;SBtfintra&#39;], inter_model=tf_blocks[&#39;SBtfinter&#39;], norm=&#39;ln&#39;, linear_layer_after_inter_intra=True, skip_around_intra=True), &quot;dec&quot;: sb.lobes.models.dual_path.Decoder(in_channels=256, out_channels=1, kernel_size=16, stride=8, bias=False)} # Training hyperparameters hparams = {&#39;num_spks&#39;: 4, &#39;lr_scheduler&#39;: sb.nnet.schedulers.ReduceLROnPlateau(factor=0.5, patience=2, dont_halve_until_epoch=1), &#39;epoch_counter&#39;: sb.utils.epoch_loop.EpochCounter(100), &#39;optimizer&#39;: lambda x: torch.optim.Adam(x, lr=0.00015), &#39;train_logger&#39;: sb.utils.train_logger.FileTrainLogger(&#39;/content/drive/MyDrive/MusDB/train_log_sepformer_modified1.txt&#39;), &#39;reconstruction_loss_weight&#39;: 0.05, &#39;similarity_loss_weight&#39;: 2.0, &#39;dissimilarity_loss_weight&#39;: 3.0, &#39;threshold_byloss&#39;: False} # Training checkpoints hparams[&#39;checkpointer&#39;] = sb.utils.checkpoints.Checkpointer(checkpoints_dir=&#39;/content/drive/MyDrive/MusDB/checkpoint_sepformer_modified4&#39;, recoverables={&#39;encoder&#39;: modules[&#39;enc&#39;], &#39;decoder&#39;: modules[&#39;dec&#39;], &#39;masknet&#39;: modules[&#39;masker&#39;], &#39;counter&#39;: hparams[&#39;epoch_counter&#39;], &#39;lr_scheduler&#39;: hparams[&#39;lr_scheduler&#39;]}) # Initialize MSS model brain = Separation(modules, hparams=hparams, opt_class=hparams[&#39;optimizer&#39;], run_opts={&quot;device&quot;:&quot;cuda:0&quot;}, checkpointer=hparams[&#39;checkpointer&#39;]) . for module in brain.modules.values(): brain.reset_layer_recursively(module) . Fitting and validation . brain.fit(hparams[&#39;epoch_counter&#39;], train_set=train_sampler, valid_set=valid_sampler) . 0%| | 0/5504 [00:00&lt;?, ?it/s] . torch.Size([1, 48000]) . 0%| | 1/5504 [00:03&lt;5:25:22, 3.55s/it, train_loss=26.4] . torch.Size([1, 48000]) . 0%| | 2/5504 [00:04&lt;3:16:41, 2.15s/it, train_loss=23] . torch.Size([1, 48000]) . 0%| | 3/5504 [00:05&lt;2:33:02, 1.67s/it, train_loss=19.4] . torch.Size([1, 48000]) . 0%| | 4/5504 [00:06&lt;2:12:28, 1.45s/it, train_loss=19.6] . torch.Size([1, 48000]) . 0%| | 5/5504 [00:08&lt;2:01:11, 1.32s/it, train_loss=20.6] . torch.Size([1, 48000]) . 0%| | 6/5504 [00:09&lt;1:54:27, 1.25s/it, train_loss=22] . torch.Size([1, 48000]) . 0%| | 7/5504 [00:10&lt;1:50:03, 1.20s/it, train_loss=19.8] . torch.Size([1, 48000]) . 0%| | 8/5504 [00:11&lt;1:47:19, 1.17s/it, train_loss=20.4] . torch.Size([1, 48000]) . 0%| | 9/5504 [00:12&lt;1:45:23, 1.15s/it, train_loss=19.5] . torch.Size([1, 48000]) . 0%| | 10/5504 [00:13&lt;1:44:23, 1.14s/it, train_loss=20.1] . torch.Size([1, 48000]) . 0%| | 11/5504 [00:14&lt;1:43:38, 1.13s/it, train_loss=19] . torch.Size([1, 48000]) . 0%| | 12/5504 [00:15&lt;1:43:06, 1.13s/it, train_loss=19.4] . torch.Size([1, 48000]) . 0%| | 13/5504 [00:16&lt;1:42:44, 1.12s/it, train_loss=18.5] . torch.Size([1, 48000]) . 0%| | 14/5504 [00:18&lt;1:42:36, 1.12s/it, train_loss=17.7] . torch.Size([1, 48000]) . 0%| | 15/5504 [00:19&lt;1:42:18, 1.12s/it, train_loss=16.9] . torch.Size([1, 48000]) . 0%| | 15/5504 [00:20&lt;2:06:10, 1.38s/it, train_loss=17.2] . KeyboardInterrupt Traceback (most recent call last) &lt;ipython-input-12-4bc14b273e15&gt; in &lt;module&gt;() -&gt; 1 brain.fit(hparams[&#39;epoch_counter&#39;], train_set=train_sampler, valid_set=valid_sampler) /content/speechbrain/speechbrain/core.py in fit(self, epoch_counter, train_set, valid_set, progressbar, train_loader_kwargs, valid_loader_kwargs) 1035 loss, self.avg_train_loss 1036 ) -&gt; 1037 t.set_postfix(train_loss=self.avg_train_loss) 1038 1039 # Debug mode only runs a few batches /usr/local/lib/python3.7/dist-packages/tqdm/std.py in set_postfix(self, ordered_dict, refresh, **kwargs) 1433 for key in postfix.keys()) 1434 if refresh: -&gt; 1435 self.refresh() 1436 1437 def set_postfix_str(self, s=&#39;&#39;, refresh=True): /usr/local/lib/python3.7/dist-packages/tqdm/std.py in refresh(self, nolock, lock_args) 1349 else: 1350 self._lock.acquire() -&gt; 1351 self.display() 1352 if not nolock: 1353 self._lock.release() /usr/local/lib/python3.7/dist-packages/tqdm/std.py in display(self, msg, pos) 1497 if pos: 1498 self.moveto(pos) -&gt; 1499 self.sp(self.__str__() if msg is None else msg) 1500 if pos: 1501 self.moveto(-pos) /usr/local/lib/python3.7/dist-packages/tqdm/std.py in print_status(s) 349 def print_status(s): 350 len_s = disp_len(s) --&gt; 351 fp_write(&#39; r&#39; + s + (&#39; &#39; * max(last_len[0] - len_s, 0))) 352 last_len[0] = len_s 353 /usr/local/lib/python3.7/dist-packages/tqdm/std.py in fp_write(s) 343 def fp_write(s): 344 fp.write(_unicode(s)) --&gt; 345 fp_flush() 346 347 last_len = [0] /usr/local/lib/python3.7/dist-packages/tqdm/utils.py in inner(*args, **kwargs) 140 def inner(*args, **kwargs): 141 try: --&gt; 142 return func(*args, **kwargs) 143 except OSError as e: 144 if e.errno != 5: /usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py in flush(self) 347 self.pub_thread.schedule(evt.set) 348 # and give a timeout to avoid --&gt; 349 if not evt.wait(self.flush_timeout): 350 # write directly to __stderr__ instead of warning because 351 # if this is happening sys.stderr may be the problem. /usr/lib/python3.7/threading.py in wait(self, timeout) 550 signaled = self._flag 551 if not signaled: --&gt; 552 signaled = self._cond.wait(timeout) 553 return signaled 554 /usr/lib/python3.7/threading.py in wait(self, timeout) 298 else: 299 if timeout &gt; 0: --&gt; 300 gotit = waiter.acquire(True, timeout) 301 else: 302 gotit = waiter.acquire(False) KeyboardInterrupt: . batch_contoh = next(iter(train_sampler)) batch_contoh[1][&#39;vocals&#39;][0].shape . torch.Size([1, 48000]) . TESTING . mix_gt_file = &#39;/content/drive/MyDrive/MusDB/Sultans of Swing/chan_6.wav&#39; drum1_gt_file = &#39;/content/drive/MyDrive/MusDB/Sultans of Swing/chan_0.wav&#39; drum2_gt_file = &#39;/content/drive/MyDrive/MusDB/Sultans of Swing/chan_1.wav&#39; drum3_gt_file = &#39;/content/drive/MyDrive/MusDB/Sultans of Swing/chan_2.wav&#39; other_gt_file = &#39;/content/drive/MyDrive/MusDB/Sultans of Swing/chan_3.wav&#39; bass_gt_file = &#39;/content/drive/MyDrive/MusDB/Sultans of Swing/chan_4.wav&#39; vocals_gt_file = &#39;/content/drive/MyDrive/MusDB/Sultans of Swing/chan_5.wav&#39; mix_gt = librosa.load(mix_gt_file, sr=8000, duration=80)[0] drum1_gt = librosa.load(drum1_gt_file, sr=8000, duration=80)[0] drum2_gt = librosa.load(drum2_gt_file, sr=8000, duration=80)[0] drum3_gt = librosa.load(drum3_gt_file, sr=8000, duration=80)[0] other_gt = librosa.load(other_gt_file, sr=8000, duration=80)[0] bass_gt = librosa.load(bass_gt_file, sr=8000, duration=80)[0] vocals_gt = librosa.load(vocals_gt_file, sr=8000, duration=80)[0] . from IPython.display import Audio Audio(mixture, rate=8000) . Your browser does not support the audio element. mix_t = torch.tensor(mix_gt).unsqueeze(0) vocals_t = torch.tensor(vocals_gt).unsqueeze(0) bass_t = torch.tensor(bass_gt).unsqueeze(0) drums_t = torch.tensor(drum1_gt + drum2_gt + drum3_gt).unsqueeze(0) other_t = torch.tensor(other_gt).unsqueeze(0) mixture = vocals_t + bass_t + drums_t + other_t targets = [vocals_t, bass_t, drums_t, other_t] with torch.no_grad(): # Perform source separation on test dataset predictions, targets, _, _ = brain.compute_forward(mixture, targets, sb.Stage.TEST) . predictions.shape . torch.Size([1, 640000, 4]) . predictions_new = torch.stack((predictions[0,:,3], predictions[0,:,1], predictions[0,:,0], predictions[0,:,2]), dim=1).unsqueeze(0) predictions_new.shape . torch.Size([1, 640000, 4]) . targets_new = torch.stack((vocals_t, bass_t, drums_t, other_t), dim=2) targets_new.shape . torch.Size([1, 640000, 4]) . Audio(predictions_new[0,:,0].cpu(), rate=8000) . Your browser does not support the audio element. Audio(targets_new[0,:,0].cpu(), rate=8000) . Your browser does not support the audio element. sdr, _, _, _ = bss_eval_sources( targets_new[0].t().cpu().numpy(), predictions_new[0].t().cpu().numpy(), ) sdr . array([4.47545751, 3.82706529, 7.70646855, 1.14874383]) . save_path = os.path.join(&quot;/content/drive/MyDrive/MusDB/Sultans of Swing&quot;, &quot;separations&quot;) # Separated audio signal if not os.path.exists(save_path): os.mkdir(save_path) for ns in range(4): # Estimated source signal = predictions_new[0, :, ns] signal = signal / signal.abs().max() save_file = os.path.join( save_path, &quot;sos_source{}hat.wav&quot;.format(ns + 1) ) torchaudio.save( save_file, signal.unsqueeze(0).cpu(), 8000 # save estimated source audio ) # Original source signal = targets_new[0, :, ns] signal = signal / signal.abs().max() save_file = os.path.join( save_path, &quot;sos_source{}.wav&quot;.format(ns + 1) ) torchaudio.save( save_file, signal.unsqueeze(0).cpu(), 8000 # save original source audio ) . Load Dataset dan DataLoader . s_rate = 8000 dur = 80 test_list = os.listdir(&#39;/content/drive/MyDrive/MusDB/wav_8k/test&#39;) root_path = Path(&#39;/content/drive/MyDrive/MusDB/wav_8k&#39;) test_dataset = MUSDB18Dataset(root_path, split=&quot;test&quot;, sample_rate=s_rate, segment=dur, random_segments=False) test_sampler = torch.utils.data.DataLoader(test_dataset, batch_size=1, **dataloader_kwargs) . 49it [01:09, 1.42s/it] . contoh = next(iter(test_sampler)) contoh . [tensor([[[-3.0518e-05, 0.0000e+00, -3.0518e-05, ..., -1.9836e-02, -2.4445e-02, -1.6846e-02]]]), {&#39;bass&#39;: tensor([[[ 0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 0.0000e+00, -3.0518e-05, -3.0518e-05]]]), &#39;drums&#39;: tensor([[[ 0.0000, 0.0000, 0.0000, ..., -0.0168, -0.0226, -0.0170]]]), &#39;other&#39;: tensor([[[ 0.0000, 0.0000, 0.0000, ..., -0.0030, -0.0018, 0.0002]]]), &#39;vocals&#39;: tensor([[[-3.0518e-05, 0.0000e+00, -3.0518e-05, ..., 0.0000e+00, 0.0000e+00, 0.0000e+00]]])}] . contoh[0].shape . torch.Size([1, 1, 640000]) . Evaluation . device = torch.device(&#39;cuda&#39;) save_file = os.path.join(&quot;/content/drive/MyDrive/MusDB&quot;, &quot;test_results_newest.csv&quot;) # SDR record save_path = os.path.join(&quot;/content/drive/MyDrive/MusDB&quot;, &quot;audio_results_newest&quot;) # Separated audio signal if not os.path.exists(save_path): os.mkdir(save_path) # Empty list of SDR and SI-SNR values all_sdrs = [] all_sdrs_vocal = [] all_sdrs_bass = [] all_sdrs_drums = [] all_sdrs_other = [] all_sdrsb_vocal = [] all_sdrsb_bass = [] all_sdrsb_drums = [] all_sdrsb_other = [] all_sdrs_b = [] all_sdrs_i = [] all_sisnrs = [] all_sisnrs_i = [] csv_columns = [&quot;song_id&quot;, &quot;sdr_vocal&quot;, &quot;sdr_bass&quot;, &quot;sdr_drums&quot;, &quot;sdr_other&quot;, &quot;sdr&quot;, &quot;sdrb_vocal&quot;, &quot;sdrb_bass&quot;, &quot;sdrb_drums&quot;, &quot;sdrb_other&quot;, &quot;sdr_b&quot;, &quot;sdr_i&quot;, &quot;si-snr&quot;, &quot;si-snr_i&quot;] with open(save_file, &quot;w&quot;) as results_csv: # write csv file for recording SDR values writer = csv.DictWriter(results_csv, fieldnames=csv_columns) writer.writeheader() # Loop over all tracks in the test data_loader with tqdm(test_sampler, dynamic_ncols=True) as t: for i, batch in enumerate(t): # Unpacking batch list mixture = batch[0][0] vocals = batch[1][&#39;vocals&#39;][0] bass = batch[1][&#39;bass&#39;][0] drums = batch[1][&#39;drums&#39;][0] other = batch[1][&#39;other&#39;][0] targets = [vocals, bass, drums, other] with torch.no_grad(): # Perform source separation on test dataset predictions, targets, _, _ = brain.compute_forward( mixture, targets, sb.Stage.TEST) for ns in range(4): # Estimated source signal = predictions[0, :, ns] signal = signal / signal.abs().max() save_file = os.path.join( save_path, &quot;item{}_source{}hat.wav&quot;.format(i + 1, ns + 1) ) torchaudio.save( save_file, signal.unsqueeze(0).cpu(), 8000 # save estimated source audio ) # Original source signal = targets[0, :, ns] signal = signal / signal.abs().max() save_file = os.path.join( save_path, &quot;item{}_source{}.wav&quot;.format(i + 1, ns + 1) ) torchaudio.save( save_file, signal.unsqueeze(0).cpu(), 8000 # save original source audio ) # Mixture signal = mixture[0, :] signal = signal / signal.abs().max() save_file = os.path.join(save_path, &quot;item{}_mix.wav&quot;.format(i + 1)) torchaudio.save( save_file, signal.unsqueeze(0).cpu(), 8000 # save mixture audio ) # Compute SI-SNR sisnr = brain.compute_objectives(predictions, targets) # Compute SI-SNR improvement mixture_signal = torch.stack( [mixture] * 4, dim=-1) mixture_signal = mixture_signal.to(device) sisnr_baseline = brain.compute_objectives( mixture_signal, targets) sisnr_i = sisnr - sisnr_baseline # Compute SDR of individual sources sdr, _, _, _ = bss_eval_sources( targets[0].t().cpu().numpy(), predictions[0].t().detach().cpu().numpy(), ) # Compute SDR improvement of individual sources over the original mixture sdr_baseline, _, _, _ = bss_eval_sources( targets[0].t().cpu().numpy(), mixture_signal[0].t().detach().cpu().numpy(), ) sdr_i = sdr.mean() - sdr_baseline.mean() # Saving SDR on a csv file row = { &quot;song_id&quot;: i+1, &quot;sdr_vocal&quot;: sdr[0], &quot;sdr_bass&quot;: sdr[1], &quot;sdr_drums&quot;: sdr[2], &quot;sdr_other&quot;: sdr[3], &quot;sdr&quot;: sdr.mean(), &quot;sdrb_vocal&quot;: sdr_baseline[0], &quot;sdrb_bass&quot;: sdr_baseline[1], &quot;sdrb_drums&quot;: sdr_baseline[2], &quot;sdrb_other&quot;: sdr_baseline[3], &quot;sdr_b&quot;: sdr_baseline.mean(), &quot;sdr_i&quot;: sdr_i, &quot;si-snr&quot;: -sisnr.item(), &quot;si-snr_i&quot;: -sisnr_i.item(), } writer.writerow(row) # Metric Accumulation all_sdrs.append(sdr.mean()) all_sdrs_vocal.append(sdr[0]) all_sdrs_bass.append(sdr[1]) all_sdrs_drums.append(sdr[2]) all_sdrs_other.append(sdr[3]) all_sdrsb_vocal.append(sdr_baseline[0]) all_sdrsb_bass.append(sdr_baseline[1]) all_sdrsb_drums.append(sdr_baseline[2]) all_sdrsb_other.append(sdr_baseline[3]) all_sdrs_b.append(sdr_baseline.mean()) all_sdrs_i.append(sdr_i.mean()) all_sisnrs.append(-sisnr.item()) all_sisnrs_i.append(-sisnr_i.item()) row = { &quot;song_id&quot;: &quot;avg&quot;, &quot;sdr_vocal&quot;: np.array(all_sdrs_vocal).mean(), &quot;sdr_bass&quot;: np.array(all_sdrs_bass).mean(), &quot;sdr_drums&quot;: np.array(all_sdrs_drums).mean(), &quot;sdr_other&quot;: np.array(all_sdrs_other).mean(), &quot;sdr&quot;: np.array(all_sdrs).mean(), &quot;sdrb_vocal&quot;: np.array(all_sdrsb_vocal).mean(), &quot;sdrb_bass&quot;: np.array(all_sdrsb_bass).mean(), &quot;sdrb_drums&quot;: np.array(all_sdrsb_drums).mean(), &quot;sdrb_other&quot;: np.array(all_sdrsb_other).mean(), &quot;sdr_b&quot;: np.array(all_sdrs_b).mean(), &quot;sdr_i&quot;: np.array(all_sdrs_i).mean(), &quot;si-snr&quot;: np.array(all_sisnrs).mean(), &quot;si-snr_i&quot;: np.array(all_sisnrs_i).mean(), } writer.writerow(row) logger = logging.getLogger(__name__) logger.info(&quot;Mean SISNR is {}&quot;.format(np.array(all_sisnrs).mean())) logger.info(&quot;Mean SISNRi is {}&quot;.format(np.array(all_sisnrs_i).mean())) logger.info(&quot;Mean SDR is {}&quot;.format(np.array(all_sdrs).mean())) logger.info(&quot;Mean SDRi is {}&quot;.format(np.array(all_sdrs_i).mean())) . 100%|██████████| 48/48 [49:29&lt;00:00, 61.87s/it] . MISCELLANEOUS . Plot train vs validation loss . import pandas as pd #header_sep = [&#39;epoch&#39;, &#39;train_sep&#39;, &#39;val_sep&#39;] df_sep = pd.read_csv(&#39;/content/drive/MyDrive/MusDB/train_log_sisnr.txt&#39;, delimiter=&#39; &#39;, header=None) df_sep = df_sep.drop(df_sep.columns[[0, 2, 3, 4, 5, 6, 8, 9, 10]], axis=1) df_sep = df_sep.replace(&#39;,&#39;,&#39;&#39;, regex=True) df_sep.columns = [&#39;epoch&#39;, &#39;train_sep&#39;, &#39;val_sep&#39;] #header_conv = [&#39;epoch&#39;, &#39;train_conv&#39;, &#39;val_conv&#39;] df_conv = pd.read_csv(&#39;/content/drive/MyDrive/MusDB/train_log_convtasnet.txt&#39;, delimiter=&#39; &#39;, header=None) df_conv = df_conv.drop(df_conv.columns[[0, 2, 3, 4, 5, 6, 8, 9, 10]], axis=1) df_conv = df_conv.replace(&#39;,&#39;,&#39;&#39;, regex=True) df_conv.columns = [&#39;epoch&#39;, &#39;train_conv&#39;, &#39;val_conv&#39;] df_sep[&#39;train_conv&#39;] = df_conv[&#39;train_conv&#39;] df_sep[&#39;val_conv&#39;] = df_conv[&#39;val_conv&#39;] df_sep . epoch train_sep val_sep train_conv val_conv . 0 1 | 10.56 | 1.950 | 9.42 | 3.000 | . 1 2 | 9.63 | 2.770 | 8.61 | 1.080 | . 2 3 | 8.45 | 2.620 | 7.22 | 3.620 | . 3 4 | 7.32 | 0.976 | 8.37 | 0.773 | . 4 5 | 9.31 | 2.490 | 8.45 | 0.501 | . ... ... | ... | ... | ... | ... | . 195 196 | 5.86 | -1.090 | 3.99 | -0.727 | . 196 197 | 6.48 | -1.110 | 5.26 | -0.506 | . 197 198 | 5.64 | 1.580 | 4.97 | -2.010 | . 198 199 | 6.16 | -1.090 | 4.39 | -2.170 | . 199 200 | 5.27 | 0.187 | 4.00 | -0.696 | . 200 rows × 5 columns . import matplotlib.pyplot as plt df_sep.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f588f4208d0&gt; .",
            "url": "https://fadelmuli.github.io/pushpop/2021/09/17/SepFormer-For-MSS.html",
            "relUrl": "/2021/09/17/SepFormer-For-MSS.html",
            "date": " • Sep 17, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://fadelmuli.github.io/pushpop/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://fadelmuli.github.io/pushpop/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://fadelmuli.github.io/pushpop/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://fadelmuli.github.io/pushpop/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}